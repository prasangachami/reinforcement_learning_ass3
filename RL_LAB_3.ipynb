{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prasangachami/reinforcement_learning_ass3/blob/master/RL_LAB_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8RUJ79B1tDLq"
      },
      "outputs": [],
      "source": [
        "# OPTIONAL INSTALLS\n",
        "# Purpose: Install required libraries if they are not already available in your environment.\n",
        "# Notes: Comment out if your environment already has these.\n",
        "\n",
        "# !pip install -q gymnasium numpy matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ENV + FRAME CAPTURE HELPERS (OFFICIAL RENDERER)\n",
        "# Purpose:\n",
        "#   - Create FrozenLake-v1 (8x8, slippery=True) with Gym/Gymnasium's official renderer\n",
        "#   - Provide helpers to roll out a policy and capture frames in notebooks\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import animation\n",
        "from IPython.display import HTML\n",
        "\n",
        "# Gym import with fallback\n",
        "try:\n",
        "    import gymnasium as gym\n",
        "except Exception:\n",
        "    import gym\n",
        "\n",
        "def make_env(seed: int = 0, is_slippery: bool = True, map_name: str = \"8x8\", render_mode: str = \"rgb_array_list\"):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "      - seed (int): Random seed for env.reset.\n",
        "      - is_slippery (bool): If True, stochastic transitions (slippery ice).\n",
        "      - map_name (str): '4x4' or '8x8' FrozenLake layout.\n",
        "      - render_mode (str): 'rgb_array_list' for notebook frames, or 'human' for a window.\n",
        "\n",
        "    Output:\n",
        "      - env (gym.Env): Initialized FrozenLake environment with the requested settings.\n",
        "    \"\"\"\n",
        "    env = gym.make(\"FrozenLake-v1\", map_name=map_name, is_slippery=is_slippery, render_mode=render_mode)\n",
        "    env.reset(seed=seed)\n",
        "    return env\n",
        "\n",
        "def random_policy_fn(env):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "      - env (gym.Env): Any Gym/Gymnasium environment with a discrete action space.\n",
        "\n",
        "    Output:\n",
        "      - policy (callable): A function policy(s) -> action that samples uniformly at random.\n",
        "    \"\"\"\n",
        "    return lambda _s: env.action_space.sample()\n",
        "\n",
        "def rollout_capture_frames(env, policy, max_steps: int = 400, seed: int | None = None):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "      - env (gym.Env): FrozenLake environment created with render_mode='rgb_array_list'.\n",
        "      - policy (callable): Function policy(s) -> action.\n",
        "      - max_steps (int): Safety cap on episode length.\n",
        "      - seed (int|None): Optional seed for env.reset.\n",
        "\n",
        "    Output:\n",
        "      - frames (list[np.ndarray]): List of RGB frames from the official renderer.\n",
        "    \"\"\"\n",
        "    if seed is not None:\n",
        "        s, info = env.reset(seed=seed)\n",
        "    else:\n",
        "        s, info = env.reset()\n",
        "\n",
        "    done = False\n",
        "    frames = []\n",
        "    while not done and len(frames) < max_steps:\n",
        "        a = policy(s)\n",
        "        s, r, terminated, truncated, info = env.step(a)\n",
        "        done = terminated or truncated\n",
        "        all_frames = env.render()  # returns all frames so far\n",
        "        if all_frames:\n",
        "            frames.append(all_frames[-1])\n",
        "    return frames\n",
        "\n",
        "def animate_official_frames(frames, title=\"FrozenLake 8×8 (slippery) — official renderer\", interval_ms: int = 140):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "      - frames (list[np.ndarray]): RGB frames (H×W×3 or 4).\n",
        "      - title (str): Title shown above the animation.\n",
        "      - interval_ms (int): Delay between frames in milliseconds.\n",
        "\n",
        "    Output:\n",
        "      - html_anim (IPython.display.HTML): HTML object rendering the animation inline.\n",
        "    \"\"\"\n",
        "    fig, ax = plt.subplots(figsize=(6, 6))\n",
        "    im = ax.imshow(frames[0])\n",
        "    ax.axis(\"off\")\n",
        "    ax.set_title(title)\n",
        "\n",
        "    def update(i):\n",
        "        im.set_data(frames[i])\n",
        "        return (im,)\n",
        "\n",
        "    ani = animation.FuncAnimation(fig, update, frames=len(frames), interval=interval_ms, blit=True)\n",
        "    plt.close(fig)\n",
        "    return HTML(ani.to_jshtml())\n"
      ],
      "metadata": {
        "id": "vXFS7jsf126y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DEMO: RANDOM WALK + ANIMATION\n",
        "# Purpose:\n",
        "#   - Demonstrate the animation\n",
        "#   - Show a 4×4 slippery random walk animation inline\n",
        "\n",
        "env_demo = make_env(seed=42, is_slippery=True, map_name=\"4x4\", render_mode=\"rgb_array_list\")\n",
        "frames_demo = rollout_capture_frames(env_demo, random_policy_fn(env_demo), max_steps=150, seed=42)\n",
        "animate_official_frames(frames_demo, \"FrozenLake 4×4 (slippery)\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UAqnzEDo209R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 1 — Understanding the Random Policy & Computing the True Value (γ = 1)\n",
        "\n",
        "**Context.**  \n",
        "In this task, we study the **uniform-random policy** on the FrozenLake-v1 environment (4×4, `is_slippery=True`).  \n",
        "Under this policy, the agent selects each of the four actions (left, down, right, up) **with equal probability (¼ each)**, regardless of its current state.  \n",
        "This makes the behavior completely random, no preference toward the goal, and it ensures that all transitions are eventually explored.\n",
        "\n",
        "With **γ = 1**, the value of each state \\( V^\\pi(s) \\) can be interpreted very intuitively:\n",
        "\n",
        "> \\( V^\\pi(s) = \\Pr_\\pi(\\text{eventually reaching the GOAL before falling into a hole }|\\; S_0=s) \\)\n",
        "\n",
        "The true value function \\(V^\\pi\\) can be computed analytically by solving a linear system over **non-terminal (transient)** states only.  \n",
        "We exclude the absorbing states (holes and goal) to make the system non-singular.\n",
        "\n",
        "---\n",
        "\n",
        "### Learning Objectives\n",
        "- Interpret \\(V^\\pi(s)\\) as the **probability of success** when γ = 1.  \n",
        "- Compute the **true value function** \\(V^\\pi\\) by solving the system \\((I - Q)v = r\\).  \n",
        "- Understand **properness**: a policy is proper if it reaches an absorbing state with probability 1.\n",
        "\n",
        "---\n",
        "\n",
        "### What You’ll Do\n",
        "1. Run the provided cell to compute the exact \\(V^\\pi\\) for the random policy.  \n",
        "2. Visualize the resulting value grid as a heatmap.  \n",
        "3. Observe how the value increases closer to the goal and remains 0 in holes and at the goal itself.\n",
        "\n",
        "---\n",
        "\n",
        "### Be Prepared to Explain\n",
        "- What the **random policy** does in FrozenLake and why it is “proper.”  \n",
        "- Why holes and the goal have a value of 0.  \n",
        "- Which states have higher success probability and why.  \n",
        "- What is the meaning of the value of a partiuclar state in this example?\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ozzbWri53FkJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================\n",
        "# Task 1 — Understanding the Policy and Computing the True Value (γ=1)\n",
        "# ==============================================================\n",
        "# Goal:\n",
        "#   - Understand the random policy being evaluated.\n",
        "#   - Compute the exact value function analytically by solving (I - P)v = r\n",
        "#     over NON-terminal (transient) states.\n",
        "#   - Interpret values as the probability of eventually reaching the goal.\n",
        "# ==============================================================\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "try:\n",
        "    import gymnasium as gym\n",
        "except Exception:\n",
        "    import gym\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "# Environment utilities\n",
        "# --------------------------------------------------------------\n",
        "\n",
        "def make_env_train(seed: int = 0, is_slippery: bool = True, map_name: str = \"4x4\"):\n",
        "    \"\"\"Create a FrozenLake-v1 environment (no renderer) for training/evaluation.\"\"\"\n",
        "    env = gym.make(\"FrozenLake-v1\", map_name=map_name, is_slippery=is_slippery)\n",
        "    env.reset(seed=seed)\n",
        "    return env\n",
        "\n",
        "def _decode_map(env):\n",
        "    \"\"\"Return grid map as list of lists of characters.\"\"\"\n",
        "    desc = env.unwrapped.desc\n",
        "    return [[c.decode(\"utf-8\") for c in row] for row in desc]\n",
        "\n",
        "def show_value_grid(V: np.ndarray, title: str = \"Value grid (4×4)\", map_name: str = \"4x4\"):\n",
        "    \"\"\"Plot value function as a 4×4 heatmap.\"\"\"\n",
        "    n = 4 if map_name == \"4x4\" else 8\n",
        "    G = V.reshape(n, n)\n",
        "    plt.figure(figsize=(3.8, 3.8))\n",
        "    plt.imshow(G, origin=\"upper\")\n",
        "    plt.colorbar(label=\"V(s)\")\n",
        "    plt.title(title)\n",
        "    for r in range(n):\n",
        "        for c in range(n):\n",
        "            plt.text(c, r, f\"{G[r,c]:.2f}\", ha=\"center\", va=\"center\", color=\"white\")\n",
        "    plt.xticks(range(n)); plt.yticks(range(n))\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "# Exact value computation for γ=1 (non-terminal linear system)\n",
        "# --------------------------------------------------------------\n",
        "\n",
        "def exact_value_for_random_policy_gamma1(env, tol: float = 1e-10):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "      - env (gym.Env): FrozenLake env; transitions read from env.unwrapped.P\n",
        "      - tol (float): Numerical tolerance for singularity/properness checks.\n",
        "\n",
        "    Output:\n",
        "      - V (np.ndarray): True state-value function under the uniform random policy (γ=1).\n",
        "                        V(s) = Probability of eventually reaching the goal from state s.\n",
        "\n",
        "    Method:\n",
        "      - Identify absorbing states (holes, goal) and remove them.\n",
        "      - Build Q and r over non-terminals under uniform random policy:\n",
        "            v = (I - Q)^(-1) * r\n",
        "      - Check if (I - Q) is invertible; if not, raise an informative error.\n",
        "    \"\"\"\n",
        "    nS = env.observation_space.n\n",
        "    nA = env.action_space.n\n",
        "    gamma = 1.0\n",
        "\n",
        "    # Identify absorbing states\n",
        "    grid = _decode_map(env)\n",
        "    n = len(grid)\n",
        "    absorbing, goal_states = set(), set()\n",
        "    for r in range(n):\n",
        "        for c in range(n):\n",
        "            idx = r * n + c\n",
        "            ch = grid[r][c]\n",
        "            if ch in (\"H\", \"G\"):\n",
        "                absorbing.add(idx)\n",
        "            if ch == \"G\":\n",
        "                goal_states.add(idx)\n",
        "\n",
        "    nonterm = [s for s in range(nS) if s not in absorbing]\n",
        "    idx_map = {s: i for i, s in enumerate(nonterm)}\n",
        "    m = len(nonterm)\n",
        "\n",
        "    Q = np.zeros((m, m), dtype=float)\n",
        "    r = np.zeros(m, dtype=float)\n",
        "    action_prob = 1.0 / nA\n",
        "\n",
        "    for s in nonterm:\n",
        "        i = idx_map[s]\n",
        "        for a in range(nA):\n",
        "            for (p, ns, rew, done) in env.unwrapped.P[s][a]:\n",
        "                if ns in absorbing:\n",
        "                    r[i] += action_prob * p * (1.0 if ns in goal_states else 0.0)\n",
        "                else:\n",
        "                    j = idx_map[ns]\n",
        "                    Q[i, j] += action_prob * p\n",
        "\n",
        "    I = np.eye(m)\n",
        "    A = I - Q\n",
        "    try:\n",
        "        eigvals = np.linalg.eigvals(Q)\n",
        "        rho = max(abs(eigvals)) if eigvals.size else 0.0\n",
        "        if rho >= 1.0 - 1e-12:\n",
        "            raise np.linalg.LinAlgError(\n",
        "                f\"(I - Q) may be singular (ρ(Q)={rho:.6f}). \"\n",
        "                \"The policy might be improper (non-absorbing).\"\n",
        "            )\n",
        "        v = np.linalg.solve(A, r)\n",
        "    except np.linalg.LinAlgError as e:\n",
        "        raise RuntimeError(\n",
        "            \"Failed to solve (I - Q)v = r for γ=1. \"\n",
        "            \"This suggests the policy may be improper (not guaranteed to reach absorption).\\n\"\n",
        "            f\"Details: {e}\"\n",
        "        )\n",
        "\n",
        "    V = np.zeros(nS)\n",
        "    for s in nonterm:\n",
        "        V[s] = v[idx_map[s]]\n",
        "    return V\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "# Compute and reflect\n",
        "# --------------------------------------------------------------\n",
        "\n",
        "env_reflect = make_env_train(seed=0, is_slippery=True, map_name=\"4x4\")\n",
        "V_true = exact_value_for_random_policy_gamma1(env_reflect)\n",
        "show_value_grid(V_true, title=\"True value\")\n",
        "\n",
        "print(\"Reflection:\")\n",
        "print(\"• Each value V(s) is the probability of eventually reaching the goal under the random policy.\")\n",
        "print(\"• Terminal states (holes, goal) have value 0 because they are absorbing.\")\n",
        "print(\"• The start state's value shows how likely a random agent is to succeed on slippery ice.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "mvh5tiXj3Hmi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2 — Monte Carlo vs TD(0) Prediction for the Random Policy (γ = 1)\n",
        "\n",
        "**Context.**  \n",
        "Now we estimate the same value function \\(V^\\pi\\) (for the **random policy**) using two learning methods:\n",
        "\n",
        "- **Monte Carlo (MC)** — estimates values by averaging the *total return* (0 or 1 in FrozenLake) over many episodes.  \n",
        "  Each update is based only on complete episodes, and updates occur once per state per episode (first-visit).  \n",
        "- **TD(0)** — learns online after every step using a bootstrapped target \\( r + γ V(s') \\).  \n",
        "  TD can learn faster initially, but its behavior depends strongly on the **step-size schedule**.\n",
        "\n",
        "Both methods are evaluated on the same random policy as in Task 1.  \n",
        "With γ = 1, the value \\(V(s)\\) again represents the **probability of eventually reaching the goal**.\n",
        "\n",
        "---\n",
        "\n",
        "### Learning Objectives\n",
        "- Observe how MC and TD(0) learn the true value function from sampled experience.  \n",
        "- Compare their convergence behavior and variance.  \n",
        "- Understand the effect of different **step-size schedules** in TD(0):  \n",
        "  - `\"constant\"`: fast early learning but noisy convergence.  \n",
        "  - `\"inverse\"`: slower updates but very stable.  \n",
        "  - `\"inverse_sqrt\"`: a balanced compromise between speed and stability.\n",
        "\n",
        "---\n",
        "\n",
        "### What You’ll Do\n",
        "1. **MC snapshots:**  \n",
        "   - Run the cell showing how the MC value grid evolves after 0, 200, 400, … episodes.  \n",
        "   - You can change `EPISODES` and `SNAPSHOT_EVERY` to experiment.  \n",
        "2. **TD(0) snapshots:**  \n",
        "   - Run the TD version and **change**  \n",
        "     ```python\n",
        "     SCHEDULE = \"inverse_sqrt\"   # or \"constant\", \"inverse\"\n",
        "     ALPHA0   = 0.1              # base step-size\n",
        "     ```  \n",
        "     to see how step-size affects learning speed and stability.  \n",
        "3. **MSE comparison:**  \n",
        "   - Run the comparison cells to see how MC and TD(0) differ in mean-squared error (MSE) as a function of episodes.\n",
        "\n",
        "---\n",
        "\n",
        "### Be Prepared to Explain\n",
        "- The main steps of the MC and TD(0) algorithms.  \n",
        "- How MC and TD(0) differ in when and how they update their estimates.  \n",
        "- Why TD(0) with a constant α may oscillate around the true values.  \n",
        "- How diminishing step-size schedules help TD(0) stabilize and reduce variance.  \n",
        "- Which method converges faster early and which one is more stable later."
      ],
      "metadata": {
        "id": "YbwooTMRDf7N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================\n",
        "# Task 2 — Monte Carlo vs TD(0) Prediction (γ=1)\n",
        "# ==============================================================\n",
        "# Goal:\n",
        "#   - Estimate V^π from sampled episodes.\n",
        "#   - Compare Monte Carlo and TD(0) learning.\n",
        "#   - Observe convergence toward the true analytical values from Task 1.\n",
        "#\n",
        "# What you need to do:\n",
        "#   - Just runt this code\n",
        "#   - Be prepered to explain what is happinging in the main steps of MC and\n",
        "#     TD(0) algorithms, thouse are clearly marked.\n",
        "# ==============================================================\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import Callable, List, Tuple\n",
        "\n",
        "def run_episode(env, policy, max_steps: int = 400, seed: int | None = None):\n",
        "    \"\"\"Run a single episode using a given policy.\"\"\"\n",
        "    if seed is not None:\n",
        "        s, info = env.reset(seed=seed)\n",
        "    else:\n",
        "        s, info = env.reset()\n",
        "    done = False\n",
        "    states, actions, rewards = [], [], []\n",
        "    while not done and len(states) < max_steps:\n",
        "        a = policy(s)\n",
        "        s2, r, terminated, truncated, info = env.step(a)\n",
        "        states.append(s); actions.append(a); rewards.append(r)\n",
        "        s = s2\n",
        "        done = terminated or truncated\n",
        "    return states, actions, rewards\n",
        "\n",
        "def random_policy_fn(env):\n",
        "    \"\"\"Uniform random policy.\"\"\"\n",
        "    return lambda _s: env.action_space.sample()\n",
        "\n",
        "# --------------------------------------------------\n",
        "#  Algorithms with progress snapshots\n",
        "# --------------------------------------------------\n",
        "\n",
        "@dataclass\n",
        "class MCPredictionResult:\n",
        "    V: np.ndarray\n",
        "    returns_count: np.ndarray\n",
        "\n",
        "def mc_prediction_first_visit_progress(env, policy: Callable[[int], int],\n",
        "                                       gamma: float = 1.0, episodes: int = 2000,\n",
        "                                       snapshot_every: int = 200) -> List[Tuple[int, np.ndarray]]:\n",
        "    \"\"\"\n",
        "    Input:\n",
        "      - env (gym.Env): FrozenLake environment (no renderer recommended for speed).\n",
        "      - policy (callable): policy(s) -> action used to generate episodes.\n",
        "      - gamma (float): Discount factor; here default is 1.0 (episodic).\n",
        "      - episodes (int): Number of episodes to sample.\n",
        "      - snapshot_every (int): Save a copy of V after every this-many episodes.\n",
        "\n",
        "    Output:\n",
        "      - snapshots (list[(int, np.ndarray)]): List of (episode_index, V_copy) pairs,\n",
        "        including (0, V_init) BEFORE any updates.\n",
        "\n",
        "    Notes:\n",
        "      - First-visit MC: update each state only at its first visit per episode.\n",
        "    \"\"\"\n",
        "    nS = env.observation_space.n\n",
        "    V = np.zeros(nS, dtype=float)\n",
        "    returns_sum = np.zeros(nS, dtype=float)\n",
        "    returns_count = np.zeros(nS, dtype=float)\n",
        "    snapshots: List[Tuple[int, np.ndarray]] = []\n",
        "\n",
        "    # Snapshot at initialization (episode 0)\n",
        "    snapshots.append((0, V.copy()))\n",
        "\n",
        "    for ep in range(1, episodes + 1):\n",
        "        states, actions, rewards = run_episode(env, policy)\n",
        "        G = 0.0\n",
        "        visited = set()\n",
        "        for t in reversed(range(len(states))):\n",
        "            s_t = states[t]\n",
        "            r_tp1 = rewards[t]\n",
        "            G = gamma * G + r_tp1\n",
        "            if s_t not in visited:\n",
        "                visited.add(s_t)\n",
        "                # =========================================================\n",
        "                # === Be prepered to expalin what is happening here =======\n",
        "                # =========================================================\n",
        "                returns_sum[s_t] += G\n",
        "                returns_count[s_t] += 1.0\n",
        "                V[s_t] = returns_sum[s_t] / returns_count[s_t]\n",
        "                # =========================================================\n",
        "                # =========================================================\n",
        "                # =========================================================\n",
        "        if ep % snapshot_every == 0:\n",
        "            snapshots.append((ep, V.copy()))\n",
        "    return snapshots\n",
        "\n",
        "def td0_prediction_progress(env, policy,\n",
        "                            gamma: float = 1.0,\n",
        "                            alpha0: float = 0.1,\n",
        "                            episodes: int = 2000,\n",
        "                            snapshot_every: int = 200,\n",
        "                            step_size_schedule: str = \"constant\"):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "      - env (gym.Env): FrozenLake environment (tabular).\n",
        "      - policy (callable): policy(s) -> action.\n",
        "      - gamma (float): Discount factor (γ=1.0 for probability-of-success view).\n",
        "      - alpha0 (float): Base step-size (interpreted per schedule).\n",
        "      - episodes (int): Number of training episodes.\n",
        "      - snapshot_every (int): Save a copy of V every this many episodes.\n",
        "      - step_size_schedule (str): One of {\"constant\", \"inverse\", \"inverse_sqrt\"}.\n",
        "         * \"constant\":      α_t(s) = alpha0\n",
        "         * \"inverse\":       α_t(s) = alpha0 / (1 + N_t(s))\n",
        "         * \"inverse_sqrt\":  α_t(s) = alpha0 / sqrt(1 + N_t(s))\n",
        "\n",
        "    Output:\n",
        "      - snapshots (list[(int, np.ndarray)]): (episode_index, V_copy) pairs,\n",
        "        including (0, V_init) BEFORE any updates.\n",
        "\n",
        "    Notes:\n",
        "      - Uses terminal-safe TD target: target = r  if done  else r + γ V[s2].\n",
        "      - We increment N(s) BEFORE computing α_t(s), so the very first visit uses 1 in the denominator.\n",
        "    \"\"\"\n",
        "    import numpy as np\n",
        "\n",
        "    # validate schedule\n",
        "    valid = {\"constant\", \"inverse\", \"inverse_sqrt\"}\n",
        "    if step_size_schedule not in valid:\n",
        "        raise ValueError(f\"step_size_schedule must be one of {valid}, got {step_size_schedule!r}\")\n",
        "\n",
        "    nS = env.observation_space.n\n",
        "    V = np.zeros(nS, dtype=float)\n",
        "    visits = np.zeros(nS, dtype=float)  # per-state visit counter\n",
        "    snapshots = [(0, V.copy())]         # include initialization\n",
        "\n",
        "    for ep in range(1, episodes + 1):\n",
        "        s, info = env.reset()\n",
        "        done = False\n",
        "        while not done:\n",
        "            a = policy(s)\n",
        "            s2, r, terminated, truncated, info = env.step(a)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            # increment visits BEFORE computing α_t(s)\n",
        "            visits[s] += 1.0\n",
        "\n",
        "            if step_size_schedule == \"constant\":\n",
        "                alpha = alpha0\n",
        "            elif step_size_schedule == \"inverse\":\n",
        "                alpha = alpha0 / (1.0 + visits[s])\n",
        "            elif step_size_schedule == \"inverse_sqrt\":\n",
        "                alpha = alpha0 / np.sqrt(1.0 + visits[s])\n",
        "\n",
        "            # =========================================================\n",
        "            # === Be prepered to expalin what is happening here =======\n",
        "            # =========================================================\n",
        "            target = r if done else (r + gamma * V[s2])\n",
        "            V[s] += alpha * (target - V[s])\n",
        "            # =========================================================\n",
        "            # =========================================================\n",
        "            # =========================================================\n",
        "\n",
        "            s = s2\n",
        "\n",
        "        if ep % snapshot_every == 0:\n",
        "            snapshots.append((ep, V.copy()))\n",
        "\n",
        "    return snapshots\n",
        "\n"
      ],
      "metadata": {
        "id": "V9aKDzAE3LH3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MC (First-Visit) SNAPSHOTS (γ=1) — initialization + reflection\n",
        "# Purpose:\n",
        "#   - Visualize how First-Visit Monte Carlo (MC) estimates the value function when γ=1\n",
        "#     (here, V(s) = probability of eventually reaching the goal under the random policy).\n",
        "#   - Include an initialization snapshot (episode 0) to show the starting point (all zeros).\n",
        "#   - Let students change only the experiment knobs: total episodes and snapshot cadence.\n",
        "#\n",
        "# Be prepared to explain what is happening:\n",
        "#   • MC collects complete episodes and, for each state's FIRST visit in an episode, uses the\n",
        "#     full return G_t (0 or 1 in FrozenLake with γ=1) to update V(s) by averaging.\n",
        "#   • Because rewards are sparse (only at the goal), successful episodes are rare at first;\n",
        "#     early estimates can stay near zero until a success occurs. As more episodes arrive,\n",
        "#     per-state sample means stabilize (variance shrinks with more visits).\n",
        "#   • The value grid typically “brightens” first near states that frequently precede the goal,\n",
        "#     then propagates as more trajectories hit diverse states before success.\n",
        "#   • Compare the result with the true value we computed in Task 1.\n",
        "#\n",
        "#\n",
        "# You can play with:\n",
        "#   - EPISODES: total training episodes (e.g., 1000–10000)\n",
        "#   - SNAPSHOT_EVERY: how often to capture snapshots (e.g., 100, 200, 500)\n",
        "\n",
        "# --- Configuration you can tweak ---\n",
        "EPISODES       = 10000\n",
        "SNAPSHOT_EVERY = 200\n",
        "\n",
        "# --- Create env and policy ---\n",
        "env_mc = make_env_train(seed=1, is_slippery=True, map_name=\"4x4\")\n",
        "policy = random_policy_fn(env_mc)\n",
        "\n",
        "# --- Run MC with snapshots (includes episode 0 initialization) ---\n",
        "mc_snaps = mc_prediction_first_visit_progress(env_mc, policy,\n",
        "                                              gamma=1.0,\n",
        "                                              episodes=EPISODES,\n",
        "                                              snapshot_every=SNAPSHOT_EVERY)\n",
        "\n",
        "# --- Show value grids for all snapshots (episode 0 included) ---\n",
        "for ep, V in mc_snaps:\n",
        "    title = (\"MC value at initialization\"\n",
        "             if ep == 0 else\n",
        "             f\"MC value after {ep} episodes\")\n",
        "    show_value_grid(V, title=title, map_name=\"4x4\")\n",
        "\n"
      ],
      "metadata": {
        "id": "Q1qFUoSXEhlZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TD(0) SNAPSHOTS (γ=1) — step-size schedules + initialization snapshot\n",
        "# Purpose:\n",
        "#   - Visualize how TD(0) learns the value function (probability of success when γ=1)\n",
        "#   - Compare different step-size schedules: \"constant\", \"inverse\", \"inverse_sqrt\"\n",
        "#   - Include an initialization snapshot (episode 0) to show the starting point (all zeros)\n",
        "#\n",
        "# Be prepared to explain what is happening:\n",
        "#   • With γ=1, V(s) estimates the probability of eventually reaching the goal (under the random policy).\n",
        "#   • TD(0) updates online using targets: target = r (if terminal) else r + γ V(s').\n",
        "#   • Step-size schedule matters:\n",
        "#       - \"constant\": α_t(s)=α0 → fast early learning but higher variance / nonzero noise floor.\n",
        "#       - \"inverse\": α_t(s)=α0/(1+N_t(s)) → slow but very stable; lowest final variance/MSE.\n",
        "#       - \"inverse_sqrt\": α_t(s)=α0/sqrt(1+N_t(s)) → compromise between speed and stability.\n",
        "#   • Compare the result with the true value we computed in Task 1.\n",
        "#\n",
        "# You can play with:\n",
        "#   - `episodes`: total training episodes (e.g., 1000–5000)\n",
        "#   - `alpha0`: base step-size (try 0.02, 0.05, 0.1, 0.2)\n",
        "#   - `snapshot_every`: how often to capture snapshots (e.g., 100, 200, 500)\n",
        "#   - `step_size_schedule`: \"constant\", \"inverse\", or \"inverse_sqrt\"\n",
        "\n",
        "# --- Configuration you can tweak ---\n",
        "EPISODES       = 2000\n",
        "ALPHA0         = 0.1\n",
        "SNAPSHOT_EVERY = 200\n",
        "SCHEDULE       = \"inverse_sqrt\"  # {\"constant\", \"inverse\", \"inverse_sqrt\"}\n",
        "\n",
        "# --- Create env and policy ---\n",
        "env_td = make_env_train(seed=2, is_slippery=True, map_name=\"4x4\")\n",
        "policy = random_policy_fn(env_td)\n",
        "\n",
        "# --- Run TD(0) with chosen schedule (includes episode 0 snapshot) ---\n",
        "td_snaps = td0_prediction_progress(env_td, policy,\n",
        "                                   gamma=1.0,\n",
        "                                   alpha0=ALPHA0,\n",
        "                                   episodes=EPISODES,\n",
        "                                   snapshot_every=SNAPSHOT_EVERY,\n",
        "                                   step_size_schedule=SCHEDULE)\n",
        "\n",
        "# --- Show value grids for all snapshots (episode 0 included) ---\n",
        "for ep, V in td_snaps:\n",
        "    title = (\"TD(0) value grid at initialization (episode 0) — γ=1\"\n",
        "             if ep == 0 else\n",
        "             f\"TD(0) value grid after {ep} episodes — γ=1 (probability of success)\")\n",
        "    show_value_grid(V, title=title, map_name=\"4x4\")"
      ],
      "metadata": {
        "id": "Dd5lyV5LcTat"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment: Compare TD(0) step-size schedules via MSE vs Episodes (γ=1)\n",
        "# Purpose:\n",
        "#   - Systematically compare how different step-size schedules affect TD(0) learning on FrozenLake 4×4.\n",
        "#   - Plot MSE (vs. the γ=1 exact value) as a function of training episodes.\n",
        "#   - Provide BOTH: (1) linear y-axis with mean ± std and a y=0 reference line,\n",
        "#                    (2) log-scale y-axis with means only (no variance band).\n",
        "#\n",
        "# Prereqs:\n",
        "#   - exact_value_for_random_policy_gamma1(env)  # γ=1, non-terminal solver with properness check\n",
        "#   - td0_prediction_progress(env, policy, ..., step_size_schedule=...)\n",
        "#   - make_env_train, random_policy_fn\n",
        "#\n",
        "# Be prepared to explain what is happening:\n",
        "#   • We measure how closely TD(0) estimates match the true V^π (probability of success) over time.\n",
        "#   • Step-size schedules trade off SPEED vs STABILITY:\n",
        "#       - \"constant\" α: faster early learning, but higher variance and a nonzero late-time noise floor.\n",
        "#       - \"inverse\"  α_t(s)=α0/(1+N_t(s)): slowest but most stable; tends to lowest final MSE.\n",
        "#       - \"inverse_sqrt\" α_t(s)=α0/sqrt(1+N_t(s)): a pragmatic middle ground (faster than inverse, steadier than constant).\n",
        "#   • We average over multiple random trials and show variability with ±1 std on a linear scale.\n",
        "#   • The log-scale figure highlights early-episode differences (means only; std bands don’t mix with logs).\n",
        "#\n",
        "# You can play with:\n",
        "#   - EPISODES: try 1000–10000\n",
        "#   - SNAPSHOT_EVERY: e.g., 100/200/500\n",
        "#   - TRIALS: more trials → smoother means/std (but slower)\n",
        "#   - TD_SETTINGS: add/remove schedules or tweak alpha0 for \"constant\"\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# -------------------------------\n",
        "# Config: episodes, trials, etc.\n",
        "# -------------------------------\n",
        "TRIALS        = 5\n",
        "EPISODES      = 10000\n",
        "SNAPSHOT_EVERY= 200\n",
        "GAMMA         = 1.0\n",
        "\n",
        "# Compare these TD(0) settings\n",
        "TD_SETTINGS = [\n",
        "    {\"label\": \"TD const α=0.20\",      \"schedule\": \"constant\",      \"alpha0\": 0.20},\n",
        "    {\"label\": \"TD const α=0.05\",      \"schedule\": \"constant\",      \"alpha0\": 0.05},\n",
        "    {\"label\": \"TD 1/(1+N)\",           \"schedule\": \"inverse\",       \"alpha0\": 1.0},\n",
        "    {\"label\": \"TD 1/sqrt(1+N)\",       \"schedule\": \"inverse_sqrt\",  \"alpha0\": 1.0},\n",
        "]\n",
        "\n",
        "def compute_mse_curve(snapshots, V_star):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "      - snapshots: list[(episode_index, V_vector)], including (0, V_init).\n",
        "      - V_star: exact value vector (γ=1).\n",
        "    Output:\n",
        "      - eps, mses: arrays aligned by episode index.\n",
        "    \"\"\"\n",
        "    eps  = np.array([ep for ep, _ in snapshots], dtype=int)\n",
        "    mses = np.array([float(np.mean((V - V_star)**2)) for _, V in snapshots], dtype=float)\n",
        "    return eps, mses\n",
        "\n",
        "def mean_std_over_trials(all_eps, all_mses):\n",
        "    \"\"\"\n",
        "    Aggregate MSE curves across trials (assumes same episode indices).\n",
        "    \"\"\"\n",
        "    eps = all_eps[0]\n",
        "    M   = np.stack(all_mses, axis=0)   # [trials, time]\n",
        "    mean= M.mean(axis=0)\n",
        "    std = M.std(axis=0, ddof=1) if M.shape[0] > 1 else M.std(axis=0)\n",
        "    return eps, mean, std\n",
        "\n",
        "def safe_for_log(y, eps=1e-12):\n",
        "    \"\"\"Clamp to avoid zeros on a log axis (purely for plotting).\"\"\"\n",
        "    return np.maximum(y, eps)\n",
        "\n",
        "# -------------------------------\n",
        "# Ground-truth V^π (γ=1)\n",
        "# -------------------------------\n",
        "env_star = make_env_train(seed=0, is_slippery=True, map_name=\"4x4\")\n",
        "V_star   = exact_value_for_random_policy_gamma1(env_star)\n",
        "\n",
        "# -------------------------------\n",
        "# Run experiments\n",
        "# -------------------------------\n",
        "results = []  # list of dicts per setting: {label, eps, mean, std}\n",
        "for i, cfg in enumerate(TD_SETTINGS):\n",
        "    eps_list, mse_list = [], []\n",
        "    for k in range(TRIALS):\n",
        "        # Stagger seeds per setting & trial\n",
        "        seed = 1000 + 100*i + k\n",
        "        env  = make_env_train(seed=seed, is_slippery=True, map_name=\"4x4\")\n",
        "        pol  = random_policy_fn(env)\n",
        "        snaps= td0_prediction_progress(env, pol,\n",
        "                                       gamma=GAMMA,\n",
        "                                       alpha0=cfg[\"alpha0\"],\n",
        "                                       episodes=EPISODES,\n",
        "                                       snapshot_every=SNAPSHOT_EVERY,\n",
        "                                       step_size_schedule=cfg[\"schedule\"])\n",
        "        e, m = compute_mse_curve(snaps, V_star)\n",
        "        eps_list.append(e); mse_list.append(m)\n",
        "    eps, mean, std = mean_std_over_trials(eps_list, mse_list)\n",
        "    results.append({\"label\": cfg[\"label\"], \"eps\": eps, \"mean\": mean, \"std\": std})\n",
        "\n",
        "# -------------------------------\n",
        "# Figure 1: Linear y-axis with ±1 std and y=0 reference line\n",
        "# -------------------------------\n",
        "plt.figure(figsize=(7.2, 4.6))\n",
        "for res in results:\n",
        "    plt.plot(res[\"eps\"], res[\"mean\"], label=f\"{res['label']} (mean)\")\n",
        "    plt.fill_between(res[\"eps\"], res[\"mean\"]-res[\"std\"], res[\"mean\"]+res[\"std\"], alpha=0.2,\n",
        "                     label=f\"{res['label']} (±1 std)\")\n",
        "plt.axhline(0.0, linestyle=\"--\")  # convergence reference\n",
        "plt.xlabel(\"Episodes\")\n",
        "plt.ylabel(\"MSE vs true V\")\n",
        "plt.title(\"TD(0) step-size schedules — FrozenLake 4×4 (slippery), γ=1  [linear scale]\")\n",
        "plt.legend(ncols=2)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# -------------------------------\n",
        "# Figure 2: Log-scale y-axis, means only\n",
        "# -------------------------------\n",
        "plt.figure(figsize=(7.2, 4.6))\n",
        "for res in results:\n",
        "    plt.plot(res[\"eps\"], safe_for_log(res[\"mean\"]), label=res[\"label\"])\n",
        "plt.yscale(\"log\")\n",
        "plt.xlabel(\"Episodes\")\n",
        "plt.ylabel(\"MSE vs true V  (log scale)\")\n",
        "plt.title(\"TD(0) step-size schedules — FrozenLake 4×4 (slippery), γ=1  [log scale]\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "QaYxKittiWP6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MC vs ONE TD(0) — make the TD step-size easy to change (γ=1)\n",
        "# Purpose:\n",
        "#   - Compare First-Visit MC against a SINGLE TD(0) configuration.\n",
        "#   - ***Students: change the TD step-size HERE*** to explore how it affects speed & stability.\n",
        "#   - Plots:\n",
        "#       (1) Linear y-axis with mean ± std and a y=0 reference line.\n",
        "#       (2) Log-scale y-axis with means only (std bands don’t mix with logs).\n",
        "#\n",
        "# Be prepared to explain what is happening:\n",
        "#   • MC (first-visit) averages full returns → unbiased; variance shrinks with visits.\n",
        "#   • TD(0) bootstraps: faster early propagation; behavior depends on step-size:\n",
        "#       - \"constant\": α_t(s)=α0 → fast start, higher variance, nonzero noise floor.\n",
        "#       - \"inverse\": α_t(s)=α0/(1+N_t(s)) → slow but very stable; lowest final MSE.\n",
        "#       - \"inverse_sqrt\": α_t(s)=α0/sqrt(1+N_t(s)) → compromise between speed and stability.\n",
        "#\n",
        "# You can play with (recommended knobs for students):\n",
        "#   - TRIALS (e.g., 3–7), EPISODES (1000–10000), SNAPSHOT_EVERY (100–500)\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# -------------------------------\n",
        "# Experiment configuration\n",
        "# -------------------------------\n",
        "TRIALS         = 5\n",
        "EPISODES       = 10000\n",
        "SNAPSHOT_EVERY = 200\n",
        "GAMMA          = 1.0\n",
        "\n",
        "# ====== YOU CAN CHANGE THIS PART ======\n",
        "TD_SCHEDULE = \"inverse\"   # one of: \"constant\", \"inverse\", \"inverse_sqrt\"\n",
        "TD_ALPHA0   = 5.0              # base step-size (meaning depends on schedule)\n",
        "# ==========================================\n",
        "\n",
        "# -------------------------------\n",
        "# Helpers (assumes earlier cells defined):\n",
        "#   - make_env_train, random_policy_fn\n",
        "#   - mc_prediction_first_visit_progress\n",
        "#   - td0_prediction_progress (supports step_size_schedule)\n",
        "#   - exact_value_for_random_policy_gamma1\n",
        "# -------------------------------\n",
        "def compute_mse_curve(snapshots, V_star):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "      - snapshots: list[(episode_index, V_vector)], includes (0, V_init)\n",
        "      - V_star: exact value vector\n",
        "    Output:\n",
        "      - eps, mses\n",
        "    \"\"\"\n",
        "    eps  = np.array([ep for ep, _ in snapshots], dtype=int)\n",
        "    mses = np.array([float(np.mean((V - V_star)**2)) for _, V in snapshots], dtype=float)\n",
        "    return eps, mses\n",
        "\n",
        "def mean_std_over_trials(all_eps, all_mses):\n",
        "    \"\"\"\n",
        "    Aggregate MSE curves across trials (assumes same episode indices).\n",
        "    \"\"\"\n",
        "    eps = all_eps[0]\n",
        "    M   = np.stack(all_mses, axis=0)\n",
        "    mean= M.mean(axis=0)\n",
        "    std = M.std(axis=0, ddof=1) if M.shape[0] > 1 else M.std(axis=0)\n",
        "    return eps, mean, std\n",
        "\n",
        "def safe_for_log(y, eps=1e-12):\n",
        "    \"\"\"Clamp to avoid zeros on a log axis (for plotting only).\"\"\"\n",
        "    return np.maximum(y, eps)\n",
        "\n",
        "# -------------------------------\n",
        "# Exact V^π (γ=1) for reference\n",
        "# -------------------------------\n",
        "env_star = make_env_train(seed=0, is_slippery=True, map_name=\"4x4\")\n",
        "V_star   = exact_value_for_random_policy_gamma1(env_star)\n",
        "\n",
        "# -------------------------------\n",
        "# MC baseline\n",
        "# -------------------------------\n",
        "mc_eps_list, mc_mses_list = [], []\n",
        "for k in range(TRIALS):\n",
        "    env_mc = make_env_train(seed=100 + k, is_slippery=True, map_name=\"4x4\")\n",
        "    pol_mc = random_policy_fn(env_mc)\n",
        "    mc_snaps = mc_prediction_first_visit_progress(env_mc, pol_mc,\n",
        "                                                  gamma=GAMMA,\n",
        "                                                  episodes=EPISODES,\n",
        "                                                  snapshot_every=SNAPSHOT_EVERY)\n",
        "    e_mc, m_mc = compute_mse_curve(mc_snaps, V_star)\n",
        "    mc_eps_list.append(e_mc); mc_mses_list.append(m_mc)\n",
        "\n",
        "eps_mc, mean_mc, std_mc = mean_std_over_trials(mc_eps_list, mc_mses_list)\n",
        "\n",
        "# -------------------------------\n",
        "# TD(0) — single configuration (students change TD_SCHEDULE/TD_ALPHA0 above)\n",
        "# -------------------------------\n",
        "td_eps_list, td_mses_list = [], []\n",
        "for k in range(TRIALS):\n",
        "    env_td = make_env_train(seed=200 + k, is_slippery=True, map_name=\"4x4\")\n",
        "    pol_td = random_policy_fn(env_td)\n",
        "    td_snaps = td0_prediction_progress(env_td, pol_td,\n",
        "                                       gamma=GAMMA,\n",
        "                                       alpha0=TD_ALPHA0,\n",
        "                                       episodes=EPISODES,\n",
        "                                       snapshot_every=SNAPSHOT_EVERY,\n",
        "                                       step_size_schedule=TD_SCHEDULE)\n",
        "    e_td, m_td = compute_mse_curve(td_snaps, V_star)\n",
        "    td_eps_list.append(e_td); td_mses_list.append(m_td)\n",
        "\n",
        "eps_td, mean_td, std_td = mean_std_over_trials(td_eps_list, td_mses_list)\n",
        "\n",
        "# -------------------------------\n",
        "# Figure 1: Linear y-axis with mean ± std, plus y=0 reference line\n",
        "# -------------------------------\n",
        "plt.figure(figsize=(7.6, 4.8))\n",
        "plt.plot(eps_mc, mean_mc, label=\"MC (mean)\")\n",
        "plt.fill_between(eps_mc, mean_mc - std_mc, mean_mc + std_mc, alpha=0.2, label=\"MC (±1 std)\")\n",
        "\n",
        "label_td = f'TD(0) {TD_SCHEDULE} (α0={TD_ALPHA0})'\n",
        "plt.plot(eps_td, mean_td, label=f\"{label_td} (mean)\")\n",
        "plt.fill_between(eps_td, mean_td - std_td, mean_td + std_td, alpha=0.2, label=f\"{label_td} (±1 std)\")\n",
        "\n",
        "plt.axhline(0.0, linestyle=\"--\")\n",
        "plt.xlabel(\"Episodes\")\n",
        "plt.ylabel(\"MSE vs true V\")\n",
        "plt.title(\"MC vs one TD(0) — change the TD step-size (FrozenLake 4×4, slippery, γ=1) [linear scale]\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# -------------------------------\n",
        "# Figure 2: Log-scale y-axis (means only)\n",
        "# -------------------------------\n",
        "plt.figure(figsize=(7.6, 4.8))\n",
        "plt.plot(eps_mc, safe_for_log(mean_mc), label=\"MC (mean)\")\n",
        "plt.plot(eps_td, safe_for_log(mean_td), label=label_td)\n",
        "plt.yscale(\"log\")\n",
        "plt.xlabel(\"Episodes\")\n",
        "plt.ylabel(\"MSE vs true V  (log scale)\")\n",
        "plt.title(\"MC vs one TD(0) — change the TD step-size [log scale]\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "d0duqcU1Eiu5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 3 — SARSA vs Q-learning (ε-greedy exploration)\n",
        "\n",
        "**Context.**  \n",
        "We now switch from *prediction* (estimating \\(V^\\pi\\)) to *control* (learning an action-value function \\(Q(s,a)\\) to improve behavior).  \n",
        "We compare two classic tabular control algorithms under **ε-greedy exploration**:\n",
        "\n",
        "- **SARSA (on-policy):** updates toward the value of the action actually taken next:\n",
        "  \\[\n",
        "  Q(S_t,A_t) \\leftarrow Q(S_t,A_t) + \\alpha \\bigl[ R_{t+1} + \\gamma\\, Q(S_{t+1}, A_{t+1}) - Q(S_t,A_t) \\bigr].\n",
        "  \\]\n",
        "- **Q-learning (off-policy):** updates toward the greedy action value:\n",
        "  \\[\n",
        "  Q(S_t,A_t) \\leftarrow Q(S_t,A_t) + \\alpha \\bigl[ R_{t+1} + \\gamma \\max_{a'} Q(S_{t+1}, a') - Q(S_t,A_t) \\bigr].\n",
        "  \\]\n",
        "\n",
        "Both *behave* with the **same ε-greedy policy** during training (exploration), but the **target** differs (on-policy vs off-policy).\n",
        "\n",
        "**Setup.**  \n",
        "- Environment: FrozenLake-v1 (4×4, `is_slippery=True`), γ = 1 (episodes terminate at holes/goal, so undiscounted returns are well-defined).  \n",
        "- Exploration: ε-greedy over \\(Q\\), with user-set ε (constant here for simplicity).\n",
        "\n",
        "**What you’ll do.**\n",
        "1. Train **SARSA** and **Q-learning** for the same number of episodes (same α and ε).  \n",
        "2. Track **success rate** (probability of reaching the goal) with a moving average.  \n",
        "3. Compare learning curves.\n",
        "\n",
        "**Be prepared to explain.**\n",
        "- The main steps off SARSA and Q-learning.  \n",
        "- How does SARSA and Q-learning differ? Which one is on-policy and which one off-policy, and why?\n",
        "- How ε and α affect speed vs stability of both algorithms.\n"
      ],
      "metadata": {
        "id": "XKt3M1OvLk38"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================\n",
        "# Task 3 — SARSA vs Q-learning (ε-greedy control): helpers\n",
        "# --------------------------------------------------------------\n",
        "# Provides:\n",
        "#   • argmax_random_tie(x, rng): choose a max index uniformly at random among ties.\n",
        "#   • epsilon_greedy_action(Q, s, epsilon, nA, rng): ε-greedy using random tie-breaking.\n",
        "#   • evaluate_success_rate(env, Q, episodes=..., epsilon_eval=0.0, seed=None):\n",
        "#       runs greedy (or ε-greedy if epsilon_eval>0) rollouts using random tie-breaking.\n",
        "#   • moving_average(x, k): simple smoothing helper for curves.\n",
        "# ==============================================================\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def argmax_random_tie(x: np.ndarray, rng: np.random.RandomState | None = None) -> int:\n",
        "    \"\"\"\n",
        "    Input:\n",
        "      - x (np.ndarray): 1D array of action values, e.g., Q[s].\n",
        "      - rng (np.random.RandomState | None): RNG for reproducibility (optional).\n",
        "    Output:\n",
        "      - idx (int): index of a maximum value, chosen uniformly at random among all max ties.\n",
        "    \"\"\"\n",
        "    if rng is None:\n",
        "        rng = np.random\n",
        "    max_val = np.max(x)\n",
        "    candidates = np.flatnonzero(x == max_val)\n",
        "    return int(rng.choice(candidates))\n",
        "\n",
        "def epsilon_greedy_action(Q: np.ndarray, s: int, epsilon: float, nA: int,\n",
        "                          rng: np.random.RandomState | None = None) -> int:\n",
        "    \"\"\"\n",
        "    Input:\n",
        "      - Q (np.ndarray): Q-table of shape [nS, nA].\n",
        "      - s (int): current state.\n",
        "      - epsilon (float): exploration probability in [0,1].\n",
        "      - nA (int): number of actions.\n",
        "      - rng (np.random.RandomState | None): RNG for reproducibility (optional).\n",
        "    Output:\n",
        "      - a (int): action selected by ε-greedy with RANDOM tie-breaking for the greedy choice.\n",
        "    \"\"\"\n",
        "    if rng is None:\n",
        "        rng = np.random\n",
        "    if rng.rand() < epsilon:\n",
        "        return int(rng.randint(nA))\n",
        "    return argmax_random_tie(Q[s], rng)\n",
        "\n",
        "def evaluate_success_rate(env, Q: np.ndarray, episodes: int = 200,\n",
        "                          epsilon_eval: float = 0.0, seed: int | None = None) -> float:\n",
        "    \"\"\"\n",
        "    Input:\n",
        "      - env: FrozenLake environment (no renderer).\n",
        "      - Q (np.ndarray): learned Q-table [nS, nA].\n",
        "      - episodes (int): number of evaluation episodes.\n",
        "      - epsilon_eval (float): ε for evaluation policy (0.0 → purely greedy).\n",
        "      - seed (int | None): RNG seed for deterministic evaluation (optional).\n",
        "    Output:\n",
        "      - success_rate (float): fraction of episodes that end in the goal (reward 1).\n",
        "    Notes:\n",
        "      - Uses RANDOM tie-breaking among greedy actions to avoid bias toward action index 0.\n",
        "    \"\"\"\n",
        "    rng = np.random.RandomState(seed) if seed is not None else np.random\n",
        "    nA = env.action_space.n\n",
        "    successes = 0\n",
        "    for _ in range(episodes):\n",
        "        s, info = env.reset()\n",
        "        done = False\n",
        "        while not done:\n",
        "            if epsilon_eval > 0.0 and rng.rand() < epsilon_eval:\n",
        "                a = int(rng.randint(nA))\n",
        "            else:\n",
        "                a = argmax_random_tie(Q[s], rng)\n",
        "            s, r, terminated, truncated, info = env.step(a)\n",
        "            done = terminated or truncated\n",
        "            if done and r == 1.0:\n",
        "                successes += 1\n",
        "    return successes / episodes\n",
        "\n",
        "def moving_average(x: np.ndarray, k: int = 50) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Input:\n",
        "      - x (np.ndarray): 1D array of values (e.g., success rates).\n",
        "      - k (int): window size (k>=1).\n",
        "    Output:\n",
        "      - y (np.ndarray): moving-average smoothed array, same length as x.\n",
        "    \"\"\"\n",
        "    k = max(1, int(k))\n",
        "    x = np.asarray(x, dtype=float)\n",
        "    if len(x) == 0:\n",
        "        return x\n",
        "    csum = np.cumsum(np.insert(x, 0, 0.0))\n",
        "    core = (csum[k:] - csum[:-k]) / k  # length = len(x)-k+1\n",
        "    y = np.empty_like(x, dtype=float)\n",
        "    # pad the first k-1 with the first computed average for a full-length series\n",
        "    y[:k-1] = core[0]\n",
        "    y[k-1:] = core\n",
        "    return y\n",
        "\n"
      ],
      "metadata": {
        "id": "7java7UcLm6L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================\n",
        "# Task 3: SARSA vs Q-learning with ε-greedy + RANDOM TIE-BREAKING\n",
        "# ==============================================================\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def train_sarsa(env, episodes=5000, alpha=0.1, epsilon=0.1, gamma=1.0, seed=0, log_every=50):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "      - env: FrozenLake env\n",
        "      - episodes (int): training episodes\n",
        "      - alpha (float): step-size\n",
        "      - epsilon (float): ε for ε-greedy behavior\n",
        "      - gamma (float): discount factor (γ=1.0 here)\n",
        "      - seed (int): RNG seed\n",
        "      - log_every (int): evaluate success rate every N episodes\n",
        "    Output:\n",
        "      - Q (np.ndarray): learned Q-table [nS, nA]\n",
        "      - logs (dict): {\"episodes\": np.ndarray, \"success\": np.ndarray}\n",
        "    Notes:\n",
        "      - On-policy target uses the ACTUAL next action a' sampled ε-greedily.\n",
        "      - RANDOM tie-breaking is used inside epsilon_greedy_action for stability.\n",
        "    \"\"\"\n",
        "    rng = np.random.RandomState(seed)\n",
        "    nS, nA = env.observation_space.n, env.action_space.n\n",
        "    Q = np.zeros((nS, nA), dtype=float)\n",
        "\n",
        "    episodes_axis, success_curve = [], []\n",
        "\n",
        "    for ep in range(1, episodes + 1):\n",
        "        s, info = env.reset()\n",
        "        a = epsilon_greedy_action(Q, s, epsilon, nA, rng)\n",
        "        done = False\n",
        "        while not done:\n",
        "            s2, r, terminated, truncated, info = env.step(a)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            if done:\n",
        "                target = r                      # terminal, no bootstrap\n",
        "                Q[s, a] += alpha * (target - Q[s, a])\n",
        "                break\n",
        "\n",
        "            a2 = epsilon_greedy_action(Q, s2, epsilon, nA, rng)\n",
        "            target = r + gamma * Q[s2, a2]     # SARSA on-policy target\n",
        "            Q[s, a] += alpha * (target - Q[s, a])\n",
        "\n",
        "            s, a = s2, a2\n",
        "\n",
        "        if ep % log_every == 0:\n",
        "            episodes_axis.append(ep)\n",
        "            success_curve.append(evaluate_success_rate(env, Q, episodes=300, epsilon_eval=0.0))\n",
        "\n",
        "    return Q, {\"episodes\": np.array(episodes_axis), \"success\": np.array(success_curve)}\n",
        "\n",
        "\n",
        "def train_qlearning(env, episodes=5000, alpha=0.1, epsilon=0.1, gamma=1.0, seed=1, log_every=50):\n",
        "    \"\"\"\n",
        "    Input/Output: same as train_sarsa.\n",
        "    Notes:\n",
        "      - Off-policy target uses max_a' Q[s', a'] (greedy), independent of next sampled action.\n",
        "      - RANDOM tie-breaking is used during ACTION SELECTION via epsilon_greedy_action.\n",
        "        (The target uses the VALUE max; tie-breaking is irrelevant there.)\n",
        "    \"\"\"\n",
        "    rng = np.random.RandomState(seed)\n",
        "    nS, nA = env.observation_space.n, env.action_space.n\n",
        "    Q = np.zeros((nS, nA), dtype=float)\n",
        "\n",
        "    episodes_axis, success_curve = [], []\n",
        "\n",
        "    for ep in range(1, episodes + 1):\n",
        "        s, info = env.reset()\n",
        "        done = False\n",
        "        while not done:\n",
        "            a = epsilon_greedy_action(Q, s, epsilon, nA, rng)\n",
        "            s2, r, terminated, truncated, info = env.step(a)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            if done:\n",
        "                target = r\n",
        "            else:\n",
        "                target = r + gamma * np.max(Q[s2])  # greedy value backup\n",
        "\n",
        "            Q[s, a] += alpha * (target - Q[s, a])\n",
        "            s = s2\n",
        "\n",
        "        if ep % log_every == 0:\n",
        "            episodes_axis.append(ep)\n",
        "            success_curve.append(evaluate_success_rate(env, Q, episodes=300, epsilon_eval=0.0))\n",
        "\n",
        "    return Q, {\"episodes\": np.array(episodes_axis), \"success\": np.array(success_curve)}\n",
        "\n",
        "\n",
        "# -------- Example run & plot (you can tweak these) --------\n",
        "GAMMA     = 1.0\n",
        "EPISODES  = 10000\n",
        "ALPHA     = 0.1\n",
        "EPSILON   = 0.1\n",
        "LOG_EVERY = 50\n",
        "SEED_SARSA= 2\n",
        "SEED_QL   = 3\n",
        "\n",
        "env3 = make_env_train(seed=123, is_slippery=True, map_name=\"4x4\")\n",
        "#env3 = make_env(seed=123, is_slippery=True, map_name=\"4x4\")\n",
        "\n",
        "Q_sarsa, log_sarsa = train_sarsa(env3, episodes=EPISODES, alpha=ALPHA, epsilon=EPSILON,\n",
        "                                 gamma=GAMMA, seed=SEED_SARSA, log_every=LOG_EVERY)\n",
        "Q_ql,    log_ql    = train_qlearning(env3, episodes=EPISODES, alpha=ALPHA, epsilon=EPSILON,\n",
        "                                     gamma=GAMMA, seed=SEED_QL,    log_every=LOG_EVERY)\n",
        "\n",
        "plt.figure(figsize=(7.2, 4.6))\n",
        "plt.plot(log_sarsa[\"episodes\"], moving_average(log_sarsa[\"success\"], k=3), label=\"SARSA (ε-greedy)\")\n",
        "plt.plot(log_ql[\"episodes\"],    moving_average(log_ql[\"success\"], k=3),    label=\"Q-learning (ε-greedy)\")\n",
        "plt.ylim(-0.05, 1.05)\n",
        "plt.xlabel(\"Training episodes\")\n",
        "plt.ylabel(\"Success rate (eval, greedy)\")\n",
        "plt.title(\"FrozenLake 4×4, slippery — SARSA vs Q-learning\")\n",
        "plt.axhline(0.0, linestyle=\"--\", linewidth=1)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "05-FRvyFMQG0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 4 — Off-Policy Q-learning from Random Behavior (Offline/Batch)\n",
        "\n",
        "**Context.**  \n",
        "We now explore the **off-policy** nature of Q-learning by separating:\n",
        "1) a **behavior policy** that generates data (the *random policy* from Tasks 1–2), and  \n",
        "2) a **target policy** implicitly defined by Q-learning (greedy w.r.t. \\(Q\\)).\n",
        "\n",
        "We will:\n",
        "- First, **collect a dataset** of transitions \\((s,a,r,s',\\text{done})\\) using the random policy (no learning).  \n",
        "- Then, run **offline Q-learning** over this fixed dataset (several epochs), updating:\n",
        "  \\[\n",
        "  Q(s,a) \\leftarrow Q(s,a) + \\alpha \\bigl[ r + \\gamma \\max_{a'} Q(s',a') - Q(s,a) \\bigr].\n",
        "  \\]\n",
        "No new interaction is used during learning — all updates come from the data buffer.  \n",
        "This demonstrates **off-policy** learning: the target (greedy) can differ from the behavior (random).\n",
        "\n",
        "**What you’ll do.**\n",
        "1. Choose a dataset size (number of random episodes) and collect data with the random policy.  \n",
        "2. Run batch/offline Q-learning for a few passes (epochs) over the dataset.  \n",
        "3. Evaluate the **greedy** policy from \\(Q\\) by its success rate.\n",
        "\n",
        "**Be prepared to explain.**\n",
        "- What is the behavioral policy here?\n",
        "- Is it a good behavioral policy? More generally, what would make a good one?\n",
        "- The algorithm only learns the optimal Q-table, not the optimal policy. How do we derive a policy from a Q-table, and how do we act greedily with respect to Q?\n",
        "- Interpret the value heatmap for the greedy policy. Is it optimal?\n"
      ],
      "metadata": {
        "id": "AHFO2v93qeQQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================\n",
        "# Task 4 — Off-policy Q-learning: convergence via true value, plus heatmaps\n",
        "# ==============================================================\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import List, Tuple\n",
        "\n",
        "Transition = Tuple[int, int, float, int, bool]  # (s, a, r, s', done)\n",
        "\n",
        "# ------------------------\n",
        "# Dataset and learning\n",
        "# ------------------------\n",
        "\n",
        "def collect_dataset_random(env, episodes: int = 1000, max_steps: int = 400, seed: int = 42) -> List[Transition]:\n",
        "    rng = np.random.RandomState(seed)\n",
        "    np.random.seed(seed)\n",
        "    nA = env.action_space.n\n",
        "    D: List[Transition] = []\n",
        "    for _ in range(episodes):\n",
        "        s, info = env.reset()\n",
        "        done = False\n",
        "        steps = 0\n",
        "        while not done and steps < max_steps:\n",
        "            a = rng.randint(nA)  # uniform random behavior\n",
        "            s2, r, terminated, truncated, info = env.step(a)\n",
        "            done = terminated or truncated\n",
        "            D.append((s, a, float(r), s2, bool(done)))\n",
        "            s = s2\n",
        "            steps += 1\n",
        "    return D\n",
        "\n",
        "def greedy_policy_array(Q: np.ndarray) -> np.ndarray:\n",
        "    return np.argmax(Q, axis=1).astype(int)\n",
        "\n",
        "def offline_q_learning_with_trace(\n",
        "    env,\n",
        "    D: List[Transition],\n",
        "    epochs: int = 25,\n",
        "    alpha: float = 0.1,\n",
        "    gamma: float = 1.0,\n",
        "    seed: int = 7,\n",
        "):\n",
        "    \"\"\"\n",
        "    Offline Q-learning over a fixed dataset.\n",
        "    Convergence metric per epoch: true start-state value under current greedy policy (γ=1).\n",
        "    \"\"\"\n",
        "    rng = np.random.RandomState(seed)\n",
        "    nS, nA = env.observation_space.n, env.action_space.n\n",
        "    Q = np.zeros((nS, nA), dtype=float)\n",
        "\n",
        "    S  = np.array([t[0] for t in D], dtype=int)\n",
        "    A  = np.array([t[1] for t in D], dtype=int)\n",
        "    R  = np.array([t[2] for t in D], dtype=float)\n",
        "    NS = np.array([t[3] for t in D], dtype=int)\n",
        "    DN = np.array([t[4] for t in D], dtype=bool)\n",
        "    idx = np.arange(len(D))\n",
        "\n",
        "    start_value_hist = []\n",
        "\n",
        "    for ep in range(epochs):\n",
        "        rng.shuffle(idx)\n",
        "        for k in idx:\n",
        "            s, a, r, ns, done = S[k], A[k], R[k], NS[k], DN[k]\n",
        "            target = r if done else (r + gamma * np.max(Q[ns]))\n",
        "            Q[s, a] += alpha * (target - Q[s, a])\n",
        "\n",
        "        # true V^π for current greedy policy; log start-state value (state 0 on 4x4)\n",
        "        pi = greedy_policy_array(Q)\n",
        "        V_pi = exact_value_for_deterministic_policy_gamma1(env, pi)\n",
        "        start_value_hist.append(float(V_pi[0]))\n",
        "\n",
        "    return Q, {\"start_value_hist\": start_value_hist}\n",
        "\n",
        "# ------------------------\n",
        "# Exact V^π for deterministic π at γ=1 (linear system over non-terminals)\n",
        "# Requires: _decode_map, env.unwrapped.P, show_value_grid available in notebook.\n",
        "# ------------------------\n",
        "\n",
        "def exact_value_for_deterministic_policy_gamma1(env, pi: np.ndarray) -> np.ndarray:\n",
        "    nS = env.observation_space.n\n",
        "    grid = _decode_map(env)  # provided elsewhere in the notebook\n",
        "    n = len(grid)\n",
        "\n",
        "    absorbing, goal_states = set(), set()\n",
        "    for r in range(n):\n",
        "        for c in range(n):\n",
        "            idx = r * n + c\n",
        "            ch = grid[r][c]\n",
        "            if ch in (\"H\", \"G\"):\n",
        "                absorbing.add(idx)\n",
        "            if ch == \"G\":\n",
        "                goal_states.add(idx)\n",
        "\n",
        "    nonterm = [s for s in range(nS) if s not in absorbing]\n",
        "    idx_map = {s: i for i, s in enumerate(nonterm)}\n",
        "    m = len(nonterm)\n",
        "\n",
        "    Qmat = np.zeros((m, m), dtype=float)\n",
        "    rvec = np.zeros(m, dtype=float)\n",
        "\n",
        "    for s in nonterm:\n",
        "        a = int(pi[s])\n",
        "        i = idx_map[s]\n",
        "        for (p, ns, rew, done) in env.unwrapped.P[s][a]:\n",
        "            if ns in absorbing:\n",
        "                rvec[i] += p * (1.0 if ns in goal_states else 0.0)\n",
        "            else:\n",
        "                j = idx_map[ns]\n",
        "                Qmat[i, j] += p\n",
        "\n",
        "    I = np.eye(m)\n",
        "    v = np.linalg.solve(I - Qmat, rvec)\n",
        "\n",
        "    V = np.zeros(nS)\n",
        "    for s in nonterm:\n",
        "        V[s] = v[idx_map[s]]\n",
        "    return V  # terminals (H,G) remain 0\n",
        "\n",
        "# ------------------------\n",
        "# Simple value heatmap from Q (uses show_value_grid for consistent style)\n",
        "# ------------------------\n",
        "\n",
        "def plot_value_heatmap_from_Q(Q: np.ndarray, title: str = \"State values from Q (max_a Q)\"):\n",
        "    Vq = np.max(Q, axis=1)\n",
        "    show_value_grid(Vq, title=title, map_name=\"4x4\")\n",
        "\n",
        "# ------------------------\n",
        "# Run\n",
        "# ------------------------\n",
        "\n",
        "GAMMA = 1.0\n",
        "EPOCHS = 10\n",
        "ALPHA = 0.1\n",
        "DATA_EPISODES = 1000\n",
        "\n",
        "env4 = make_env_train(seed=2024, is_slippery=True, map_name=\"4x4\")\n",
        "\n",
        "# 1) Fixed behavior dataset from a random policy\n",
        "dataset = collect_dataset_random(env4, episodes=DATA_EPISODES, max_steps=400, seed=42)\n",
        "print(f\"Collected transitions: {len(dataset)}\")\n",
        "\n",
        "# 2) Offline Q-learning with convergence via true start-state value\n",
        "Q_off, trace = offline_q_learning_with_trace(\n",
        "    env4, dataset,\n",
        "    epochs=EPOCHS, alpha=ALPHA, gamma=GAMMA, seed=7\n",
        ")\n",
        "\n",
        "# 3) Convergence plot: TRUE start-state value of greedy policy over epochs (γ=1)\n",
        "plt.figure(figsize=(4.4, 3.2))\n",
        "plt.plot(trace[\"start_value_hist\"], marker='o')\n",
        "plt.xlabel(\"Epoch\"); plt.ylabel(\"Start-state V^π\")\n",
        "plt.title(\"Convergence via true value of greedy policy\")\n",
        "plt.tight_layout(); plt.show()\n",
        "\n",
        "# 4) Heatmaps\n",
        "pi_final = greedy_policy_array(Q_off)\n",
        "V_pi_final = exact_value_for_deterministic_policy_gamma1(env4, pi_final)\n",
        "show_value_grid(V_pi_final, title=\"True V^π of final greedy policy\", map_name=\"4x4\")\n"
      ],
      "metadata": {
        "id": "d7fKUkUQqphC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}